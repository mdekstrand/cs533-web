
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Week 13 ‚Äî Unsupervised (11/15‚Äì19) &#8212; CS 533 Fall 2021</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/fonts.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/tabinsert.js"></script>
    <script src="../_static/urlclean.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "mdekstrand/cs533-web");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="copyright" title="Copyright" href="../copyright/" />
    <link rel="next" title="Week 14 ‚Äî Workflow (11/29‚Äì12/3)" href="../week14/" />
    <link rel="prev" title="Week 12 ‚Äî Text (11/8‚Äì12)" href="../week12/" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../">
      
      
      
      <h1 class="site-logo" id="site-title">CS 533 Fall 2021</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search/" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../">
   CS 533 Homepage
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Course Info
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../syllabus/">
   Syllabus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../schedule/">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../glossary/">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Content
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../week0/">
   Week 0 ‚Äî Pre-Class Welcome
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week1/">
   Week 1 ‚Äî Questions (8/23‚Äì27)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week1/DemoNotebook/">
     Demo Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week1/1-7-types-operations/">
     Data Types and Operations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week2/">
   Week 2 ‚Äî Description (8/30‚Äì9/3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/2-2-PandasBasics/">
     Introducing Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/2-4-AggregatesAndGroups/">
     Aggregates and Groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week2/2-6-DescribingDistributions/">
     Describing Distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../week3/">
   Week 3 ‚Äî Presentation (9/6‚Äì10)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week3/3-6-ChartsFromTheGroundUp/">
     Drawing Charts
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week4/">
   Week 4 ‚Äî Inference (9/13‚Äì17)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week5/">
   Week 5 ‚Äî Filling In (9/20‚Äì24)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week6/">
   Week 6 ‚Äî Two Variables (Sep. 27‚ÄìOct. 1)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week7/">
   Week 7 ‚Äî Getting Data (Oct. 4‚Äì8)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week8/">
   Week 8 ‚Äî Regression (Oct. 11‚Äì15)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week9/">
   Week 9 ‚Äî Models &amp; Prediction (Oct. 18‚Äì22)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week10/">
   Week 10 ‚Äî Classification (10/25‚Äì29)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week11/">
   Week 11 ‚Äî More Modeling (11/1‚Äì5)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week12/">
   Week 12 ‚Äî Text (11/8‚Äì12)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Week 13 ‚Äî Unsupervised (11/15‚Äì19)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week14/">
   Week 14 ‚Äî Workflow (11/29‚Äì12/3)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../week15/">
   Week 15 ‚Äî What Next? (12/6‚Äì10)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/">
   Rubric
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../assignments/A0/">
   Assignment 0
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/A0/A0-Notebook/">
     CS 533 Assignment 0
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A1/">
   Assignment 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A2/">
   Assignment 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A3/">
   Assignment 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A4/">
   Assignment 4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A5/">
   Assignment 5
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A6/">
   Assignment 6
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/A7/">
   Assignment 7
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/software/">
   Software and Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/documentation/">
   Documentation &amp; Reading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/data/">
   Data Sets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/notebook-checklist/">
   Notebook Checklist &amp; Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/probability/">
   Notes on Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/problems/">
   Common Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/onyx/">
   Remotely Using Onyx
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/git-resources/">
   Git Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../resources/environments/">
   Software Environments
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../resources/tutorials/">
   Tutorials
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/AdvancedPipeline/">
     Advanced Pipeline Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/BooleanSeries/">
     Tricks with Boolean Series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/BuildingData/">
     Building Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/ChartFinishingTouches/">
     Finishing Touches
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Charting/">
     Drawing Charts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/ClusteringExample/">
     Clustering Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Correlation/">
     Correlation and Basic Linear Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/CriticScores/">
     Charting Movie Scores
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Distributions/">
     Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/EmpiricalProbabilities/">
     Empirical Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/FetchCHIPapers/">
     Bibliography Fetch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/FunWithNumbers/">
     Fun with Numbers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Functions/">
     Writing Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Indexing/">
     Indexing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/LogitRegressionDemo/">
     Logistic Regression Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/MLTimeSeries/">
     MovieLens Time Series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/MinimizeRegression/">
     Rebuilding Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/MissingData/">
     Missing Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/MovieDecomp/">
     Movie Matrix Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/OneSample/">
     One Sample
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/OverfittingSimulation/">
     Overfitting Simulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/PCADemo/">
     PCA Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/PenguinSamples/">
     Sampling and Testing the Penguins
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Regressions/">
     Regressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Reshaping/">
     Reshaping Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SamplingDists/">
     Sampling Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SciKitLogistic/">
     SciKit Logistic Regression Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SciKitPipeline/">
     SciKit Pipeline and Transform Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SciKitRegression/">
     SciKit-Learn Linear Regression Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SciKitTransform/">
     SciKit Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Selection/">
     Selecting Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/Sessions/">
     Sessionization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/SpamFilter/">
     Spam Detector Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/TuningExample/">
     Tuning Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resources/tutorials/UsingTheCensus/">
     Using Census Data
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Site Details
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../copyright/">
   Copyright and License
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/week13/index.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/mdekstrand/cs533-web"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/mdekstrand/cs533-web/issues/new?title=Issue%20on%20page%20%2Fweek13/index.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/mdekstrand/cs533-web/edit/main/week13/index.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#content-overview">
   üßê Content Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deadlines">
   üìÖ Deadlines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#no-supervision">
   üé• No Supervision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decomposing-matrices">
   üé• Decomposing Matrices
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#resources">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#movie-decomposition">
   üìì Movie Decomposition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   üé• Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering-example">
   üìì Clustering Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-spaces">
   üé• Vector Spaces
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-and-entropy">
   üé• Information and Entropy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#week-13-quiz">
   üö© Week 13 Quiz
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practice-svd-on-paper-abstracts">
   üìì Practice: SVD on Paper Abstracts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assignment-6">
   üì© Assignment 6
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="week-13-unsupervised-11-1519">
<span id="week13"></span><h1>Week 13 ‚Äî Unsupervised (11/15‚Äì19)<a class="headerlink" href="#week-13-unsupervised-11-1519" title="Permalink to this headline">¬∂</a></h1>
<p>In this week, we are going to talk more about <em>unsupervised learning</em> ‚Äî learning without labels.
We are not going to have time to investigate these techniques very deeply, but I want you to know about them, and you are experimenting with them in Assignment 6.</p>
<p>This week‚Äôs content is lighter, since we just had a large assignment and a midterm, and another assignment is due on Sunday.</p>
<div class="section" id="content-overview">
<h2>üßê Content Overview<a class="headerlink" href="#content-overview" title="Permalink to this headline">¬∂</a></h2>
<div class="module-content docutils">
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><strong>Element</strong></th>
<th class="head"><strong>Length</strong></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#7fb7ef35-8f6d-482b-8dfd-adc60183d110"><span>üé•¬†No Supervision</span></a></p></td>
<td><span>2m51s</span></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#3d617c61-5cb1-48d9-ba65-adc60183d1b2"><span>üé•¬†Decomposing Matrices</span></a></p></td>
<td><span>17m22s</span></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cc647448-7dca-4cb6-ae10-adc60183d23f"><span>üé•¬†Clustering</span></a></p></td>
<td><span>6m56s</span></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#58ec551f-d458-40bc-9ee3-adc60183d2cb"><span>üé•¬†Vectors and Spaces</span></a></p></td>
<td><span>7m27s</span></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#2c3c3aab-643d-4b2c-b18b-addf00076faf"><span>üé•¬†Information and Entropy</span></a></p></td>
<td><span>10m31s</span></td>
</tr>
</tbody>
</table>
<p><span>This week has </span><strong>0h45m</strong><span> of video and </span><strong>0 words</strong><span> of assigned readings.</span><span> This week‚Äôs videos are available in a </span><a class="panopto reference external" href="https://boisestate.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%220ad54e44-0249-4039-b405-adc601833667%22"><span>Panopto folder</span></a><span> and as a </span><a class="panopto podcast reference external" href="https://boisestate.hosted.panopto.com/Panopto/Podcast/Podcast.ashx?courseid=0ad54e44-0249-4039-b405-adc601833667&amp;type=mp4"><span>podcast</span></a><span>.</span></p>
</div>
</div>
<div class="section" id="deadlines">
<h2>üìÖ Deadlines<a class="headerlink" href="#deadlines" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p>Quiz 13, November 18</p></li>
<li><p><a class="reference internal" href="../assignments/A6/"><span class="doc std std-doc">Assignment 6</span></a>, November 21</p></li>
</ul>
</div>
<div class="section" id="no-supervision">
<h2>üé• No Supervision<a class="headerlink" href="#no-supervision" title="Permalink to this headline">¬∂</a></h2>
<p>In this video, we review the idea of supervised learning and contrast it with unsupervised learning.</p>
<div class="resource video docutils container" id="7fb7ef35-8f6d-482b-8dfd-adc60183d110">
<div class="tabbed-set docutils">
<input checked="checked" id="327d9f14-73e3-42f8-8a72-d1ba746d1bc8" name="4b97d4d2-20e9-467d-bd75-37c0da751824" type="radio">
</input><label class="tabbed-label" for="327d9f14-73e3-42f8-8a72-d1ba746d1bc8">
Video (2m51s)</label><div class="tabbed-content player docutils">
<div class="video-container video-embed">
<iframe src="https://boisestate.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=7fb7ef35-8f6d-482b-8dfd-adc60183d110&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>
</div></div>
<input id="d707b5b7-5451-4076-8ea5-9db4dd124066" name="4b97d4d2-20e9-467d-bd75-37c0da751824" type="radio">
</input><label class="tabbed-label" for="d707b5b7-5451-4076-8ea5-9db4dd124066">
Slides</label><div class="tabbed-content slides docutils">
<div class=slide-container data-id="495979F9A431DDB0!74469" data-key=AHTnV7d9LFdqwF4>
</div></div>
</div>
<div class="slides text hidden docutils">
CS 533INTRO TO DATA SCIENCE
Michael Ekstrand
NO SUPERVISION
Learning Outcomes (Week)
Distinguish between supervised and unsupervised learning.
Project data into lower-dimensional space with matrix factorization.
Cluster data points.
Photo by Benedikt Geyer on Unsplash
Learning So Far
We learn to predict a label
Categorical label ‚Üí classification
Continuous label ‚Üí regression

This is called supervised learning
We have ground truth for outcome
Sometimes called supervision signal
Unsupervised Learning
What can we do without a supervision signal?
Group instances together (clustering)
Learn vector spaces for items
Learn relationships between items
Learn relationships between features
Middle Ground: Self-Supervised Learning
Sometimes we can extract supervision signals from data
Word embeddings: predict if two words appear together

Why?
Exploring data
Reducing data complexity
For visualization
For learning (‚Äúcurse of dimensionality‚Äù)
Inputs into other models
Sometimes it‚Äôs all we have
Wrapping Up
Unsupervised learning learns patterns from data without labels.

It‚Äôs useful for grouping items together, exploration, and as input to other models.
Photo by Fran Jacquier on Unsplash
</div>
<div class="captions text hidden docutils">
<ul class="simple">
<li>But in this video, I'm going to introduce you to the idea of unsupervised learning.</li>
<li>This week we're going to learn about the difference between supervised and unsupervised learning.</li>
<li>You're going to learn how to project data into lower dimensional spaces with matrix factorization and to cluster data points.</li>
<li>So so far, we've been focusing on learning what we're trying to predict a label.</li>
<li>We have a categorical label where we're trying to predict. And this is classification we're trying to classify as spam, not spam fraud.</li>
<li>There's a couple of the examples we've been using.</li>
<li>We can have a continuous label we're trying to predict, in which case we call it, in which case it's three regression.</li>
<li>But this is called we can also try to predict ordinal variables, et cetera.</li>
<li>But this is all called supervised learning. Where we have the key idea here is we have a ground truth for the outcome.</li>
<li>We have observed outcomes for our training data. This is sometimes called a supervisions signal.</li>
<li>And we're trying to learn to predict these known outcomes.</li>
<li>That's that's the heart of what it means to do supervised learning. But.</li>
<li>We can do things without having a supervision signal.</li>
<li>And some of the things we can do without access to a supervision signals, we can try to group instances together what's called clustering,</li>
<li>where we try to find related groups and clustering and multiclass classification are related.</li>
<li>Because if you've got multiple class labels, then you're trying to divide them into that.</li>
<li>Clustering is where you're trying to buy the Met, but you don't have the class labels.</li>
<li>You can try to learn vector spaces for items in order to say,</li>
<li>learn the relationship between items in some cases, also to learn the relationships between features of items.</li>
<li>There's also a middle ground called self supervised learning where you don't have labels in the sense that we use them in supervised learning,</li>
<li>but you extract something that looks like a label from the data and use that as a supervisions signal.</li>
<li>Word and beddings are one example of of self supervised learning.</li>
<li>So why do we want to do this unsupervised learning? There's a few reasons.</li>
<li>One is that it can be useful as a data exploration tool.</li>
<li>If you can find clusters in the data, then that can help guide where you your investigation to understand what's going on in your data.</li>
<li>It can help to reduce data complexity for either visualization or for subsequent learning tasks.</li>
<li>You can use them as inputs into other models and sometimes it's all we have.</li>
<li>We don't have access to labels and we're trying to make sense of our data source. Unsupervised learning techniques can be helpful in order to do that.</li>
<li>So to wrap up unsupervised learning learns patterns from data.</li>
<li>We don't have labels available. It's useful for grouping items together, exploration and as input into other models.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="decomposing-matrices">
<h2>üé• Decomposing Matrices<a class="headerlink" href="#decomposing-matrices" title="Permalink to this headline">¬∂</a></h2>
<p>This video introduces the idea of <em>matrix decomposition</em>, which we can use to <em>reduce the dimensionality</em> of data points.</p>
<div class="resource video docutils container" id="3d617c61-5cb1-48d9-ba65-adc60183d1b2">
<span id="index-0"></span><div class="tabbed-set docutils">
<input checked="checked" id="ac541095-b4e4-44b5-b4ac-02cf2911d37e" name="d70f79b8-5748-4f06-a208-ebf074cf76e8" type="radio">
</input><label class="tabbed-label" for="ac541095-b4e4-44b5-b4ac-02cf2911d37e">
Video (17m22s)</label><div class="tabbed-content player docutils">
<div class="video-container video-embed">
<iframe src="https://boisestate.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=3d617c61-5cb1-48d9-ba65-adc60183d1b2&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>
</div></div>
<input id="a5cabff9-f36a-4d4e-91d2-d881cac9002e" name="d70f79b8-5748-4f06-a208-ebf074cf76e8" type="radio">
</input><label class="tabbed-label" for="a5cabff9-f36a-4d4e-91d2-d881cac9002e">
Slides</label><div class="tabbed-content slides docutils">
<div class=slide-container data-id="495979F9A431DDB0!74472" data-key=ANGtrO-Yt2wmGUs>
</div></div>
</div>
<div class="slides text hidden docutils">
CS 533INTRO TO DATA SCIENCE
Michael Ekstrand
DECOMPOSING MATRICES
Learning Outcomes
Review matrix multiplication
Decompose a matrix into a lower-rank approximation
Photo by Carissa Weiser on Unsplash
What Is a Matrix?
Matrix Multiplication
Sparse Matrix
A matrix is sparse (mathematically) if most values are 0.

Sparse matrix representations only store nonzero values
scipy.sparse

np.ndarray is our dense matrix
DataFrame and Series cannot be sparse üòî (they store 0s)
Dimensionality Reduction
Why?
Compact representation
Remove noise from original matrix
Plot high-dimensional data to show relationships
SVD preserves distance
SVD can improve distance
Find relationships between features
Principle Component Analysis ‚Äì find vectors of highest variance
How?
Principal Component Analysis
Use Case 1: Compression &amp; Denoising
Use Case 2: Visualization
Low-dimensional vectors can be visualized!
See example notebooks

Use Case 3: Better Neighborhoods
High-dimensional spaces have 2 problems for distance:
Distance more expensive to compute
Points approach equidistant in high-dimensional space

Decomposed matrices can improve this!
k-NN classification
k-means clustering
Use Case 4: Categorical Interactions
Wrapping Up
Matrix decomposition (also called matrix factorization or dimensionality reduction) breaks a high-dimensional matrix into a low-dimensional one.

It preserves distance and, in some configurations, finds the direction of maximum variance.
Photo by Thomas Willmott on Unsplash
</div>
<div class="captions text hidden docutils">
<ul class="simple">
<li>Blow in this video. I want to introduce the idea of Matrix decompositions.</li>
<li>There's a couple of notebooks that go with this to demonstrate the concepts more and to give you some additional readings in this.</li>
<li>This video is going to explain what's going on. So goal here is to view matrix multiplication and decompose a matrix into a lower rank approximation.</li>
<li>So if you've taken a linear algebra class, you may have seen a matrix.</li>
<li>We have a it's a two matrix is just a two dimensional array of numbers.</li>
<li>We say it's dimension is M by N Rose always go first. When we're notating matrices, this is also the convention used by NUM Pi.</li>
<li>And so it is. It's Rose R and dimensional row vectors.</li>
<li>So that's a row vector. It's columns. R m dimensional column vectors have a column vector there.</li>
<li>We can also compute its transpose if we swapped the rows and columns. Vampyre exposes that as the exact capital T operation.</li>
<li>But a matrix is it's this two dimensional array of numbers. We can do a few things with them.</li>
<li>We can add them together. We can subtract them. One of those things we can do is we can multiply them.</li>
<li>So if we have two matrices A and B and A is M by K and B is K by N.</li>
<li>This is important. The inner dimensions of the two matrices have to match.</li>
<li>Then we can compute the Matrix product and it's gonna be M times and so for multiplying two matrices.</li>
<li>Unlike multiplication of of scalars.</li>
<li>Matrix multiplication is not commutative. You can't switch B get the same result.</li>
<li>You have to have the same the same matrix or the same dimensionality on the inside.</li>
<li>And what you get as the result is the dimensionality of the outside.</li>
<li>So what it's defined as is is CIJ Row, Row I column J is defined by the SARM.</li>
<li>Across the row of A and down the column of B of the pairwise items.</li>
<li>So it is it is the DOT product of Roe A.</li>
<li>And Column B. Or and of column Jay of of Matrixx Base.</li>
<li>You compute. You compute the DOT product. So see what C is, is it is the dot product of every row of A with every column of B num pi.</li>
<li>You can compute this with the A and B operation. That's the Python Matrix multiplication operator.</li>
<li>So this is a fundamental operation for matrices that you can multiply them together.</li>
<li>We also can have what we call sparse matrices and a matrix is sparse.</li>
<li>If most of its values are zero, that's what it means mathematically for it to be sparse.</li>
<li>Computationally a sparse matrix is a matrix with a zero. Values are not stored.</li>
<li>And so Saipov provide the sparse Matrix class that we can a number of sparse matrix classes that we can use.</li>
<li>The number high ENDI array is a dense matrix. Data frame is also a dense matrix.</li>
<li>S data frame.</li>
<li>It's very serious, can't be sparse, but if we need to do sparse computations, we can use this Sipi that sparse package to give us sparse matrices.</li>
<li>This is what Saikat learned does under the hood.</li>
<li>When you do, when you tokenized text with its with its count vector Dreiser or its TFI D.F. Vector riser,</li>
<li>its giving use Sibai sparse matrices as a result.</li>
<li>Now one of the things we can do with another thing we can do with a matrix is do what's called the dimensionality reduction.</li>
<li>And this follows from a theorem that if we have a matrix and intown by hand,</li>
<li>then we can compute a decompositions into the multiplication of three matrices P Sigma and Q Transpose.</li>
<li>And this gives us we can break down any matrix into this, Sipi provides us with functions in order to compute this decompensating given an ax.</li>
<li>It will compute piece Sigmon Q or Q Transpose. We can also then truncate this.</li>
<li>So Sigma is what's called the singular values.</li>
<li>We can truncate this matrix, only keep the K largest ones and set the rest of zero or just cut them out so that.</li>
<li>To so that we can get we get a narrower P and a narrower cure, a shorter Q transpose.</li>
<li>And this gives us an approximation of the original Matrix X.</li>
<li>There's a few useful properties. So the rows of P rows of P correspond to rows of X rows correspond.</li>
<li>So what Pier gives us is a K dimensional. Representation of rows of X, if X has a lot of columns.</li>
<li>This is super useful because if case more than no columns of X, then we get the smaller,</li>
<li>more compact representation of the original rows of the Matrix.</li>
<li>Also, it preserves distance things that are things are approximately as far apart in the in P as they are in the original X.</li>
<li>So why do we want to do this? One reason is for a compact representation. As I said, we get this k dimensional representation of our values x.</li>
<li>It can be useful to remove noise. I'll talk more about that in a little bit.</li>
<li>It can be useful for plotting high dimensional data to show relationships. If you've got 50 columns of X.</li>
<li>You can plot just like two columns of it. Or you can take the SDD to find columns that are particularly hard to project it into another.</li>
<li>Another vector space that you can show it in two dimensions that maximize the maximize the</li>
<li>amount or the extent to which the data points you can be spread out in those two dimensions.</li>
<li>It can also improve our ability to compute distances and fine and then it can provide.</li>
<li>It finds relationship and it can be helpful for finding relationships between features.</li>
<li>So if we have correlated features, we have multiple features that are partially measuring a similar thing.</li>
<li>They're correlated with each other. Principal component analysis is an application of matrix composition that allows us to find those relationships</li>
<li>and combine those correlated things and extract non correlated components out of these correlated observations.</li>
<li>So how do you actually do this? So Saikat learn provide a truncated SPDM class.</li>
<li>It's a transformer. If you call fit, it learns Kute transpose.</li>
<li>If you call transformatory turns the rows of P for the instances you pass in the anthraces,</li>
<li>you pass and don't have to be the same instances that you gave to fit.</li>
<li>Fit. Transform does the whole thing at once. I'm giving you example Kobel that you see this in action.</li>
<li>And then there's also the SEVIS function inside PI that computes the SVOD of a sparse matrix.</li>
<li>So one of the applications of SFD, as I said, is something called principal component analysis.</li>
<li>If you mean center, your features are you can standardize them. But if you mean center your features and then you compute the S.V. D.</li>
<li>What you get is the columns of P or what we call principle components.</li>
<li>So columns zero. Is the position of the data point along a vector that has a maximum variance and you're over?</li>
<li>You can go over to Q and find that vector in the original of the original data space.</li>
<li>And so what it does is it finds you've got this you've got this data in space and it finds it aligned through the data.</li>
<li>That explains more variance over a long which is not that it explains more</li>
<li>variance along which there is more variance than there is along any other line.</li>
<li>You could draw through that data point through that that space. That can be the axis at axis and a vector space.</li>
<li>And then if you see projekt all of your points onto that line, then you can find another line that explains most of the remaining.</li>
<li>The more of the remaining variance than any other.</li>
<li>So here I have I have data projected in two dimensional space, it's actually three dimensional data.</li>
<li>There's some correlation. We get this line here that runs through it.</li>
<li>And this line, if you go along so that the variance along the axis,</li>
<li>there's a fair amount of variance is a fair amount of variance along why there's more variance along this line.</li>
<li>So it gives just this line that this is the line through which there's most of the variance.</li>
<li>We could transform the data. So this line is now our X X-axis.</li>
<li>And then we could look at where's the where's the rest of the variance? So we can see here.</li>
<li>Here I'm showing the vectors the first and the second principle components.</li>
<li>The first one is along this line. I showed you the first place. It can go either way.</li>
<li>PCI does not guarantee which direction the sign is going to go. It does the same flip.</li>
<li>You can point the arrow the other direction, but you've got this this vector here that lets us.</li>
<li>Is this line along which there's more variance than any other?</li>
<li>But then there's this second line and it's orthogonal to the first and it's OK, where's the next chunk of variance?</li>
<li>What direction do I go to find the next amount of variance? You and The Notebook was online that generated these plots along with the simulation.</li>
<li>You can play with a little bit. So why do we want to use this? There's a few different useful use cases.</li>
<li>One is to compress and genoise our data. So as I said, we truncate we can truncate DVD.</li>
<li>We keep the K largest singular values. This means that P and Q are much smaller than pretty liste.</li>
<li>P is much smaller than X.</li>
<li>Then the result is that when we multiply them back together, it approximates X and it is it is the best rank approximation to the rank.</li>
<li>The rank of a matrix is basically a measure of how complex it is.</li>
<li>What it is, is it's the number of non-zero values in the singular value decomposition.</li>
<li>And so if we zero out the smallest values, what we get is.</li>
<li>If least squares error is our measure. Of how good an approximation of the original matrix is.</li>
<li>There is no better approximation than the truncated SPDM.</li>
<li>Another thing that happens is if there's noise in X, if X is some strong signal and a bunch of noise.</li>
<li>The largest singular values and singular vectors are probably going to pick up the signal and not the noise.</li>
<li>Always both for the most part. And so if you add the noise will be learned in the smaller vectors.</li>
<li>And so if you drop the smaller vectors, then you're dropping a bunch of the noise.</li>
<li>And so it can be useful to clean up data for some various purposes.</li>
<li>If X is sparsely observed,</li>
<li>you can use this also to impute values if you're careful about how you set up the composition because that he got but this you have to have</li>
<li>the full matrix if you're careful about how you set it up or use an alternative means of learning one that can deal with missing data.</li>
<li>Then you can multiply them back together to predict what the values you weren't able to observe of X are.</li>
<li>Really useful technique for imputing that data.</li>
<li>And for filling in unobserved values.</li>
<li>This is how a lot of recommender systems work. Actually, if we observe your preference for some movies,</li>
<li>we can use a singular value decomposition or a derivative of it in order to fill back in and estimate your preference for the movies you haven't seen.</li>
<li>And then if X is the document term Matrix or the Roeser documents and the columns or terms and we take the SFD,</li>
<li>this is what's called latent semantic analysis or latent semantic indexing.</li>
<li>And it's a way for understanding.</li>
<li>What we call the topics in a corpus, because these these dimensions and in the reduced dimensionality space, the metric, the.</li>
<li>This inner vector space and talk of another video about what a little bit more about what that means.</li>
<li>They correspond theoretically to different kinds of topics.</li>
<li>And so if that document becomes represented rather than the words, it becomes represented as a vector over topics.</li>
<li>And each document is a mixture of these topics and words correspond to topics as well.</li>
<li>The model there is that a document produces a word or contains a word because the document is about topic and the word is relevant to the topic.</li>
<li>And so you learn these topics.</li>
<li>And it lets you compare documents even if they don't have as many words in common because you can establish this in enemies, OK?</li>
<li>These words are on the same topic. Then if I use some of them, I'm on that topic.</li>
<li>And another document uses other ones that it's on that topic. And we can learn the topic relationship by doing a matrix, the composition.</li>
<li>Another one is for visualization. So low dimensional vectors can be visualized.</li>
<li>And I show this in the example notebooks.</li>
<li>But if we take an SVOD, then we can you say the first two columns of the SFD to visualize our data points in a space.</li>
<li>The space is not human interpretable. But let's see how spread out the points are.</li>
<li>We can also use it to get better neighborhood. So one of the problems, there's a couple of problems with high dimensional spaces.</li>
<li>We're trying to compute distances.</li>
<li>One is that distance is more expensive to compute because the more dimensions you have, the more compute you need to do.</li>
<li>But also, as the dimensionality of a space increases, the number of features,</li>
<li>the number of columns in your in your Matrix point start to look about the same distance from each other.</li>
<li>It's called the cursive dimensionality. Decomposed matrices can help with this.</li>
<li>So doing SFD can help make either a K and then classifier or a commune's clustering approach work better if you work on the.</li>
<li>If you do the K and N or the K means clustering, which we're going to talk about in the next video.</li>
<li>On top of. The transformed data using an SVOD, it can sometimes be more effective than if you just use it on its own.</li>
<li>The fourth case is the model categorical interaction. So if we want to models say the likelihoods of words to appear together,</li>
<li>like what's the likelihood that apple and fish appear within three words of each other in a sentence?</li>
<li>We can think about this as a probability, but there's N squared of them because we have no probability for every pair of words.</li>
<li>That's a lot to learn if we were going to learn.</li>
<li>If you want to learn a matrix that maps the probability between every pair of words in the English language, that's a very, very large matrix.</li>
<li>So instead, what we can do is we can learn of reduced dimensionality, space, and we usually don't do this by actually taking the NCD.</li>
<li>We do it with with approximation method that just directly optimize these vectors.</li>
<li>But we can learn vectors for words so that.</li>
<li>You basically using a logistic model of the probabilities so that the DOT products between them is the law gods of the two words appearing together.</li>
<li>And so words that appear together are going to have similar vectors.</li>
<li>Words that appear far apart are going to have very different vectors. And this is called a word embedding.</li>
<li>This is what a word embedding does. Like word the vac glove. These various word embedded.</li>
<li>This is what they do. And more sophisticated versions of this are at the heart of a lot of machine learning models.</li>
<li>So a lot of neural architecture is a lot of deep learning.</li>
<li>Models have various embedding and all in embedding is it's a vector representation of something.</li>
<li>And they're often done through these kinds of dimensionality, reduction techniques or approximations of them,</li>
<li>so that you get you get these vectors, these low dimensional vectors that are in a space like they're 10 dimensional vector.</li>
<li>And the 10 dimensions don't mean anything. They're just dimensions that are useful for explaining this.</li>
<li>This this instance is relationship to whatever we're trying to do with it.</li>
<li>And so they take you a long ways and a lot of machine learning.</li>
<li>And then they're the core piece of a lot of different models.</li>
<li>So to wrap up Matrix decompositions, which is also called a matrix factorization or dimensionality reduction,</li>
<li>breaks a high dimensional matrix into a low dimensional one. And it's useful for compressing data.</li>
<li>You've got a more compact representation. It's useful for making it more well behaved numerically.</li>
<li>We can compute better distances. We compute distances more efficiently, can reduce noise in the data.</li>
<li>There's a lot of different purposes for which decomposing data into this lower dimensional space is super useful.</li>
</ul>
</div>
</div>
<div class="section" id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p>The next notebook</p></li>
<li><p>The <a class="reference internal" href="../resources/tutorials/PCADemo/"><span class="doc std std-doc">PCADemo</span></a>, demonstrating the PCA plots</p></li>
<li><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.23)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse" title="(in SciPy v1.9.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scipy.sparse</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html#scipy.linalg.svd" title="(in SciPy v1.9.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.linalg.svd()</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html#scipy.sparse.linalg.svds" title="(in SciPy v1.9.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.sparse.linalg.svds()</span></code></a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.decomposition.TruncatedSVD</span></code></a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.decomposition.PCA</span></code></a></p></li>
</ul>
</div>
</div>
<div class="section" id="movie-decomposition">
<h2>üìì Movie Decomposition<a class="headerlink" href="#movie-decomposition" title="Permalink to this headline">¬∂</a></h2>
<p>The <a class="reference internal" href="../resources/tutorials/MovieDecomp/"><span class="doc std std-doc">Movie Decomposition</span></a> notebook demonstrates matrix decomposition with movie data.</p>
</div>
<div class="section" id="clustering">
<h2>üé• Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¬∂</a></h2>
<p>This video introduces the concept of <em>clustering</em>, another useful unsupervised learning technique.</p>
<div class="resource video docutils container" id="cc647448-7dca-4cb6-ae10-adc60183d23f">
<div class="tabbed-set docutils">
<input checked="checked" id="6aee7f37-e5b2-4e34-be96-c41e4d4f1296" name="f87d104f-8da6-43d2-96ea-0947db867286" type="radio">
</input><label class="tabbed-label" for="6aee7f37-e5b2-4e34-be96-c41e4d4f1296">
Video (6m56s)</label><div class="tabbed-content player docutils">
<div class="video-container video-embed">
<iframe src="https://boisestate.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=cc647448-7dca-4cb6-ae10-adc60183d23f&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>
</div></div>
<input id="70774fbf-4a9f-4727-a48c-7b8da16c4af0" name="f87d104f-8da6-43d2-96ea-0947db867286" type="radio">
</input><label class="tabbed-label" for="70774fbf-4a9f-4727-a48c-7b8da16c4af0">
Slides</label><div class="tabbed-content slides docutils">
<div class=slide-container data-id="495979F9A431DDB0!74471" data-key=AFUjHnI7FLIz3CE>
</div></div>
</div>
<div class="slides text hidden docutils">
CS 533INTRO TO DATA SCIENCE
Michael Ekstrand
CLUSTERING
Learning Outcomes
Understand the idea of ‚Äòclustering‚Äô
Interpret the results of clustering with k-means
Photo by Markus Winkler on Unsplash
Grouping Things Together
What if we want to find groups in our data points?

We don‚Äôt know the groups (or we would classify)
Find them from the data

This is clustering
Membership Kinds
Mixed-membership: point can be in more than one cluster
Matrix factorization can be a kind of clustering

Single-membership: point is in precisely one cluster
Centroid-Based Clustering
K-Means Algorithm
Clustering in SKlearn
KMeans class
fit(X) learns cluster centers (can take y but will ignore)
predict(X) maps data points to cluster numbers
cluster_centers_ has cluster centers (in input space)

Other clustering algorithms have similar interface.
Evaluating Clusters
Look at them
Seriously. Look at them.
If you have labels, compare
Useful for understanding behavior
Quality scores
E.g. silhouette compares inter- and intra-cluster distances
Can be used to compare clusterings, no absolute quality values
Wrapping Up
Clustering allows us to identify groups of items from the data.

May or may not make sense.

Cluster quality depends on features, metric, cluster count, and more.
Photo by Igor Milicevic on Unsplash
</div>
<div class="captions text hidden docutils">
<ul class="simple">
<li>This video I want to introduce clustering, so learning outcomes are for.</li>
<li>To understand the idea of clustering and to interpret the results of clustering with K means.</li>
<li>So the idea of clustering is to group things together. So if we want to find groups in our data points, but we don't know what the groups are,</li>
<li>many clustering techniques require us to know how many groups there are. But we don't know what the groups are.</li>
<li>If we did, we would just use a multiclass classifier to find them. We want to find them from the data.</li>
<li>This is what we call clustering. So there's a couple of different kinds of clustering in terms of the membership of the clusters.</li>
<li>One is mixed membership where a point can be in more than one cluster.</li>
<li>And it has a different degree of affinity for the different clusters matrix factorization we can see as a kind of mixed membership clustering.</li>
<li>Where do the values in the decomposed and the lower dimensional space are?</li>
<li>How strongly the matrix is associated? The data point is associated with that cluster, but single membership clustering we want to find.</li>
<li>We want to find clusters. And we want to put each point in one cluster. So we might have movie types, different types of movies.</li>
<li>Want to put each movie in a different type. These might align with genres. They might align with something else.</li>
<li>So the idea one technique is to do it based on what we call centroid and the centroid is just the center of a cluster.</li>
<li>And so to do this, we typically need a distance function between two data points, between two vectors.</li>
<li>Often this is the Euclidean distance, but we have to define the vector space properly.</li>
<li>We need to do the feature engineering, have the features appropriately normalized and standardized so that the distance between them.</li>
<li>The distance between their vectors actually reflects how far apart the vectors are, the instances are with regards to our clustering goal.</li>
<li>If the distance does not relay, if it isn't so, that more similar items in terms of what we what we hope the cluster is going to uncover,</li>
<li>those more similar items need to have a smaller distance between each other than they do their distance to a not a less similar right along again,</li>
<li>along whatever it is that we hope the clustering is going to uncover.</li>
<li>We can do clustering on on dimensioned after dimensionality reductions that we can get.</li>
<li>We can get our ah. We can work in a lower dimensional space and sometimes that'll make where our distances be better behaved.</li>
<li>So the goal is to find the centroid of these clusters. And then what we'll do is we'll when an item comes in, we'll find which of our clusters.</li>
<li>So we have 10 clusters. We're gonna fi compare it. We're gonna measure its distance from the centers of all of the clusters.</li>
<li>And we're gonna say it's in the closest one. And so the K means algorithm does this by.</li>
<li>So we tell you how many clusters we want. We want five clusters, 10 clusters.</li>
<li>And it picks ten points. And says, these are my cluster centers.</li>
<li>And then figures out what cluster all of the data points are in.</li>
<li>And now that it's got all the data points clustered, it uses the it takes each cluster and recompute the new center.</li>
<li>It takes all the data points, computes the center of that set of points. And that's the new cluster center.</li>
<li>It then does this again because then you move the cluster center.</li>
<li>It might be that some points on the edge between it and another cluster switch clusters.</li>
<li>And then once you've switched clusters, you compute the center centroid again and you repeat this several times until what we call convergence.</li>
<li>And so this is a this is an example. We've seen a couple of others of what's called an iterative method,</li>
<li>iterative method as a method where you start somewhere and you incrementally improve your result.</li>
<li>So we start with some cluster centers, cluster the data points, move the centers to reflect the data points.</li>
<li>Try again. And convergence basically means it stops moving.</li>
<li>We've rerun another round of it. And our cluster centers haven't moved very much.</li>
<li>This does require us to know. Okay. It can't figure out how many clusters there are supposed to be.</li>
<li>And they're optimizations that can improve it various ways, particularly picking a like the simple way to do it.</li>
<li>If you pick K points just completely at random. There are more sophisticated ways to pick those points that can result in better clustering behavior.</li>
<li>So to do this, since I can't learn the K means class or do K means clustering fit learns, the cluster centers predict,</li>
<li>will map a data point to a cluster no super give it predict with some data and it will give you numbers, cluster numbers, cluster centers.</li>
<li>If you look if you go into and get the cluster centers attribute out of the escolar an object, that's the center centroid.</li>
<li>If your clusters of other clustering algorithms and Saikat learn of a similar interface.</li>
<li>Now we've got these clusters. How do we see how well they work?</li>
<li>Well, look at them like the purpose here is we want to uncover data, uncover connections and groupings in the data.</li>
<li>But we don't have labels, so one thing you really have to do with clustering is just look at it.</li>
<li>Do the clusters seem to be finding coherent? Do they seem to be finding coherent sets of things that we're clustering?</li>
<li>If you do have labels, sometimes we will have labels and we can like we have labels for a little bit of data.</li>
<li>We can use it to compare clustering behaviors, clustering systems also, or cluster clustering results.</li>
<li>Also, it can be useful when we're experimenting with clustering techniques to cluster data</li>
<li>where we do have the labels to see how good a job it's doing at recovering labels.</li>
<li>And we do have them to get some idea of how it might do. And we don't have them.</li>
<li>And then there are some quality scores.</li>
<li>There's a score called silhouette that compares the distances within a cluster to the distances between items and items and the closest other cluster.</li>
<li>And if things tend to be closer to each other than they are to other clusters, then you've got a better clustering.</li>
<li>These can be used to compare clustering, but there's not an absolute quality value like,</li>
<li>oh, a silhouette of point five means they've got a good clustering. No clustering is a really, really evaluating.</li>
<li>Clustering is a really imprecise thing. But the basic it basically is the clustering useful for what you're trying to do with it.</li>
<li>So to wrap up clustering allows us to identify groups of items in the data. These clusters may or may not make sense.</li>
<li>You have to look at them really. Cluster quality depends on a number of things.</li>
<li>Your features and your metric are super important because if you don't have a feature space and a</li>
<li>metric such that things that are similar to each other are close together in the on your metric,</li>
<li>then clustering is not going to be able to find the relationships you're looking for. Also, the cluster counters superimportant.</li>
<li>If there are eight natural groups and you try to find five clusters, clusters might not work so well.</li>
<li>Now the natural gropings and the cluster count do not necessarily need a map.</li>
<li>Sometimes you can get good cluster rings with an extra cluster or not having quite as many clusters.</li>
</ul>
</div>
</div>
<div class="section" id="id1">
<h3>Resources<a class="headerlink" href="#id1" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.cluster.KMeans</span></code></a></p></li>
</ul>
</div>
</div>
<div class="section" id="clustering-example">
<h2>üìì Clustering Example<a class="headerlink" href="#clustering-example" title="Permalink to this headline">¬∂</a></h2>
<p>The <a class="reference internal" href="../resources/tutorials/ClusteringExample/"><span class="doc std std-doc">clustering example notebook</span></a> shows how to use the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> class.</p>
</div>
<div class="section" id="vector-spaces">
<h2>üé• Vector Spaces<a class="headerlink" href="#vector-spaces" title="Permalink to this headline">¬∂</a></h2>
<p>This video talks about <em>vector spaces</em> and transforms.</p>
<div class="resource video docutils container" id="58ec551f-d458-40bc-9ee3-adc60183d2cb">
<div class="tabbed-set docutils">
<input checked="checked" id="7cab060c-4cb9-4bf0-9a76-3d33ef9c601d" name="54f514b6-f10d-4f58-949b-dc01ee365fa5" type="radio">
</input><label class="tabbed-label" for="7cab060c-4cb9-4bf0-9a76-3d33ef9c601d">
Video (7m27s)</label><div class="tabbed-content player docutils">
<div class="video-container video-embed">
<iframe src="https://boisestate.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=58ec551f-d458-40bc-9ee3-adc60183d2cb&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>
</div></div>
<input id="b87b3e72-2ac3-43e5-afdd-af3da3d10c5b" name="54f514b6-f10d-4f58-949b-dc01ee365fa5" type="radio">
</input><label class="tabbed-label" for="b87b3e72-2ac3-43e5-afdd-af3da3d10c5b">
Slides</label><div class="tabbed-content slides docutils">
<div class=slide-container data-id="495979F9A431DDB0!74470" data-key=AC4gQFrydXurq-g>
</div></div>
</div>
<div class="slides text hidden docutils">
CS 533INTRO TO DATA SCIENCE
Michael Ekstrand
VECTOR SPACES
Learning Outcomes
Introduce more formally the concept of a vector space
Understand vector space transformations
Photo by Markus Winkler on Unsplash
Vector Spaces
Vector Operations
Addition (and subtraction)
Scalar multiplication
Inner products (sum of elementwise products)
Distance (inner product of subtraction with itself)
Matrix of Data Points
What Is A Matrix?
A collection of row vectors
A collection of column vectors
A linear map from one vector space to another

A few matrix ops:
Addition
Multiplication (by scalar or compatible matrix or vector)
Transpose
Special Matrices
Matrix-Vector Multiplication
Transformations
All by multiplying by a matrix:
Reduce (or increase) dimensionality
Translate
Scale
Skew
Rotate
Any linear transformation (this is actually what linear means)
Linear Systems
Wrapping Up
Vectors represent data points in a vector space.

These can be manipulated and transformed.

Linear algebra teaches much more.
Photo by Jurica Koletiƒá on Unsplash
</div>
<div class="captions text hidden docutils">
<ul class="simple">
<li>This video, I want to talk a little bit more about vector spaces, we've talked about them a little,</li>
<li>but we're going to talk about the concept in a little more detail now.</li>
<li>Want to formally introduce more formally introduced the concept of vector space and understand the idea of a vector space transformation.</li>
<li>We're only going to scratch the surface for a lot more. I recommend that you read a good linear algebra book.</li>
<li>So remember, vector is a vector is there's a sequence of numbers, an array, basically a one dimensional array.</li>
<li>So X X one X two X and that's the vector real's to the end is an end dimensional vector space.</li>
<li>In the real numbers, you vector spaces over other things to integers over.</li>
<li>Complex numbers over weird things, but it's an n dimensional vector space and we can do a few operations, the vector.</li>
<li>We can add, subtract, we can multiply by a scalar. That's a real number, we can compute the inner product.</li>
<li>You cannot multiply multiplying two vectors, just I'm going to multiply defector's together is not an operation.</li>
<li>If you both apply to vectors and know what you're actually getting is the pairwise</li>
<li>multiplication like it multiplies the elements together if they're compatible sizes.</li>
<li>That is not actually a linear algebra operation. There is the inner product, which is the sum of the element Y's products.</li>
<li>And then there's a distance, which is the inner product of a subtraction with itself. And so we can have a matrix.</li>
<li>So we've got a sample exit rows and instance, each instance is a row vector.</li>
<li>As a row of this matrix X, we can do all the vector things with these rows.</li>
<li>Matrix, a matrix is, as we said in an earlier video. Is this this two dimensional array?</li>
<li>Of numbers. It's a collection of row vectors and it's a collection of Cullom vectors.</li>
<li>It's also a linear map from one vector space to another. And the other one might be the same the the the same vector space in terms like it</li>
<li>might be n by n subtle map are an N dimensional vector to another N dimensional vector.</li>
<li>But it's had some transformations applied to it. There's a bunch of things we can do with matrices.</li>
<li>We can add them, multiply them by either a scalar or by a compatible matrix or vector.</li>
<li>We saw that earlier. You can transpose them, et cetera. There's a number of special matrices.</li>
<li>So we have column vectors which are m by one, remember, we always have rows first.</li>
<li>So M rose by one column is an M by one column vector.</li>
<li>One row by N columns is an it is a one by N row vector.</li>
<li>We can have a square matrix where the two dimensions are the same.</li>
<li>We can have a diagonal matrix that only had that zero everywhere except the diagonal.</li>
<li>So you've got your big matrix. It's got. The diagonal is non-zero.</li>
<li>All of this is zero. You can have an identity matrix, which is a diagonal matrix where all the non-zero values are one.</li>
<li>You can have a triangular matrix where either the upper right. Or the lower left corner is non-zero and the other side is zero.</li>
<li>So it's the everything above and to the right of the diagonal is zero for a lower triangular matrix.</li>
<li>Everything down the left of the diagonal is zero for an upper triangular matrix.</li>
<li>Also a symmetric matrix, which is a square matrix where it's equal to its transpose.</li>
<li>So the top right corner is equal to the top bottom left corner.</li>
<li>You flip it. You flip the rows in the column and you get the same Matrix backout. You can also have what's called an orthogonal matrix,</li>
<li>where if a transpose times A is equal to the identity matrix, then you have an orthogonal matrix.</li>
<li>Matrix vector multiplication is super useful operation.</li>
<li>So if we've got an M byan matrix and we've gotten N dimensional column vector, then we can compute Y equals X and we can.</li>
<li>And this is going to be an M dimensional column vector.</li>
<li>And what we've done here is we have mapped X into another vector space or we've transformed it.</li>
<li>And even if even if the even if a a square. So it's from R and R n.</li>
<li>What we're doing we can apply this can transform the vector so that it's in it's in the same space.</li>
<li>But it is its relationships to other vectors have changed.</li>
<li>And it's effectively a. In the different organization of the same space for lack of a better term.</li>
<li>I'm trying to avoid getting deep into the linear algebra terms like so like change of basis and things.</li>
<li>Because I'm trying to give you the intuition for it.</li>
<li>And linear algebra class is gonna really eat either a class or a textbook or an online course is going</li>
<li>to help you shore up a lot of the details that you're going to need to dove deeper into linear algebra.</li>
<li>So multiplying by a matrix can give us a bunch of different transform.</li>
<li>We can reduce dimensionality. We can basically project, project and do other transformations.</li>
<li>So a projection is when you just strip off vectors. So if we have X.</li>
<li>So if we've got one seven. The projection of this under the first dimension is just one.</li>
<li>But you can also do some additional transformations at your besides just projection to get it down to a lower dimensional space.</li>
<li>One of the things you can do is translate. So if we've got here. We can translate it.</li>
<li>We've got Vector's. And we just shift them, same relationship to each other.</li>
<li>They're just moved. We can scale them so that they're going to this vector is going to.</li>
<li>Get a vector here. We can scale it.</li>
<li>You can skew the space. You can also rotate within the space and any combination of these, you can do any linear transformation.</li>
<li>And actually this is what it means for something to be a linear transformation of linear</li>
<li>transformation is the transformation you can express through a matrix multiplication.</li>
<li>But also, we have linear systems, so linear systems are written as matrix vector operations.</li>
<li>We can solve this for Beda. So why is X better than Beda is equal is X inverse times Y.</li>
<li>If we want ordinarily, this is this is the direct exact solution to the linear equations.</li>
<li>But if they don't have a solution, we can get the least squared solution by solving a different system.</li>
<li>Multiply. X transpo solve, X transpose, Y equal to X transpose, x beda.</li>
<li>I missed it x there, I just wrote it in.</li>
<li>Now one particular note though is so I wrote a matrix inverse here matrix and versus an operation but usually don't actually want to perform it.</li>
<li>Matrix and Versus are almost always used for solving a system.</li>
<li>Linear equation solving the system is usually a better solution than actually inverting a matrix.</li>
<li>So wrap up vectors represent data points in a vector space. These can be manipulated and transform, particularly by multiplying them by a matrix.</li>
<li>I recommend that you can salties Lynge some linear algebra learning resources to learn a lot more.</li>
</ul>
</div>
</div>
<div class="section" id="id2">
<h3>Resources<a class="headerlink" href="#id2" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://linear.axler.net/"><cite>Linear Algebra Done Right</cite></a> by Sheldon Axler</p></li>
<li><p><cite>Handbook of Linear Algebra</cite> (terse and comprehensive reference)</p></li>
</ul>
</div>
</div>
<div class="section" id="information-and-entropy">
<h2>üé• Information and Entropy<a class="headerlink" href="#information-and-entropy" title="Permalink to this headline">¬∂</a></h2>
<p>This video introduces the idea of <em>entropy</em> as a way to quantify information.  It‚Äôs something I want to make sure you‚Äôve seen
at least once by the end of the class.</p>
<div class="resource video docutils container" id="2c3c3aab-643d-4b2c-b18b-addf00076faf">
<div class="tabbed-set docutils">
<input checked="checked" id="912bc0f1-ac5c-4c0e-82a4-8ce54fb7dff4" name="262c51e9-09d4-4a0b-9662-24a037fbcf8c" type="radio">
</input><label class="tabbed-label" for="912bc0f1-ac5c-4c0e-82a4-8ce54fb7dff4">
Video (10m31s)</label><div class="tabbed-content player docutils">
<div class="video-container video-embed">
<iframe src="https://boisestate.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=2c3c3aab-643d-4b2c-b18b-addf00076faf&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay"></iframe>
</div></div>
<input id="0eebdc46-a386-4396-a7d9-61023f4e2827" name="262c51e9-09d4-4a0b-9662-24a037fbcf8c" type="radio">
</input><label class="tabbed-label" for="0eebdc46-a386-4396-a7d9-61023f4e2827">
Slides</label><div class="tabbed-content slides docutils">
<div class=slide-container data-id="495979F9A431DDB0!74497" data-key=AI2F3oJ76E5sN_U>
</div></div>
</div>
</div>
<div class="section" id="id3">
<h3>Resources<a class="headerlink" href="#id3" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://www.worldcat.org/oclc/1170834662"><cite>An Introduction to Information Theory: Symbols, Signals &amp; Noise</cite></a> by John R. Pierce</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy (information theory)</a> on Wikipedia</p></li>
</ul>
</div>
</div>
<div class="section" id="week-13-quiz">
<h2>üö© Week 13 Quiz<a class="headerlink" href="#week-13-quiz" title="Permalink to this headline">¬∂</a></h2>
<p>Take the Week 13 quiz on Canvas.</p>
</div>
<div class="section" id="practice-svd-on-paper-abstracts">
<h2>üìì Practice: SVD on Paper Abstracts<a class="headerlink" href="#practice-svd-on-paper-abstracts" title="Permalink to this headline">¬∂</a></h2>
<p>The <span class="xref myst">Week 13 Exercise notebook</span> demonstrates latent semantic analysis on paper abstracts and has an exercise to classify text into new or old papers.</p>
<p>It requires the <a class="reference download internal" download="" href="../_downloads/2fa87c465dcb7cb3e0298b0e7525bced/chi-papers.csv"><code class="xref download docutils literal notranslate"><span class="pre">chi-papers.csv</span></code></a> file, which is derived from the <a class="reference external" href="http://hcibib.org">HCI Bibliography</a>.
It is the abstracts from papers published at the CHI conference (the primary conference for human-computer interaction) over a period of nearly 40 years.</p>
<p>If you want to see how to create this file, see the <a class="reference internal" href="../resources/tutorials/FetchCHIPapers/"><span class="doc std std-doc">Fetch CHI Papers example</span></a>.</p>
</div>
<div class="section" id="assignment-6">
<h2>üì© Assignment 6<a class="headerlink" href="#assignment-6" title="Permalink to this headline">¬∂</a></h2>
<p><a class="reference internal" href="../assignments/A6/"><span class="doc std std-doc">Assignment 6</span></a> is due <strong>November 21</strong>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./week13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../week12/" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Week 12 ‚Äî Text (11/8‚Äì12)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../week14/" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 14 ‚Äî Workflow (11/29‚Äì12/3)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Michael D. Ekstrand<br/>
        
            &copy; <a href="../copyright/">Copyright</a> 2021.<br/>
          <div class="extra_footer">
            <script data-goatcounter="https://cs533.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>